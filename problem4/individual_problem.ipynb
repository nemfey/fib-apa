{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4n_30wpwAc8"
      },
      "source": [
        "**Autor**: Victor Teixidó López"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GDzH8y8zwFdM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voHRN4q2t9mL"
      },
      "source": [
        "# ME QUEDO CON TU CARA\n",
        "\n",
        "Las Olivetti faces son un conjunto de datos clásico de reconocimiento de imágenes. Es una colección de 400 imágenes de 40 personas diferentes de tamaño 64×64. El objetivo es obtener un clasificador capaz de etiquetar correctamente las imágenes. Vamos a utilizar la librería Tensorflow de Google a través de la API Keras, que simplifica la definición de arquitecturas de redes neuronales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "looEgUXwJCG3"
      },
      "outputs": [],
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iBlBpuIt9pv"
      },
      "source": [
        "## Apartado a\n",
        "\n",
        "El primero paso va a ser obtener los datos. Los cogeremos de la librería scikit-learn a través de la función fetch_olivetti_faces que devolverá dos matrices de datos, una para los datos de entrada y otra para las etiquetas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urAt6uimuS5p",
        "outputId": "977be747-1c93-4c73-d8e3-6de5ca0b6e13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading Olivetti faces from https://ndownloader.figshare.com/files/5976027 to /root/scikit_learn_data\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "Xn, yn = fetch_olivetti_faces(return_X_y=True);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSnN-0XUzYmx"
      },
      "source": [
        "### División de los datos\n",
        "\n",
        "Dividimos los datos en conjuntos de entrenamiento y test (70%/30%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xck3yLzmxh66"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Xn, yn, train_size=0.7, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY84AtgjzVMX"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Después de dividir los datos en los conjuntos de entrenamiento y test, aplicaremos a ambos conjuntos un pequeño preproceso. El primer proceso consiste en normalizar los datos para que estén en el rango [0-1].\n",
        "\n",
        "Debido a que Keras no distingue entre clasificación y regresión como lo hace scikit learn, dado que la tarea se define por la función de pérdida que se utiliza. Este es un problema multiclase, por lo que la función de pérdida que corresponde es la entropía cruzada categórica. Los problemas multiclase, se resuelven mediante una regresión multisalida que genera la probabilidad para cada clase mediante la función softmax. Es por esto que transformaremos las etiquetas del problema en una matriz con codificación one-hot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg2J1p7i0dp8"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def preprocessing(X, y):\n",
        "    print('Tamaño original:{}'.format(X.shape))\n",
        "    \n",
        "    # normalization of the data [0-1]\n",
        "    X = X / np.max(Xn)\n",
        "\n",
        "    # target classes transformation\n",
        "    encoder = OneHotEncoder(sparse='False')\n",
        "    tarjet_encoding = encoder.fit(y.reshape(-1,1))\n",
        "\n",
        "    y = tarjet_encoding.fit_transform(y.reshape(-1,1)).toarray()\n",
        "    \n",
        "    print('Nuevo tamaño:{}'.format(X.shape))\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhoershX0rtF",
        "outputId": "860d9f26-8eec-4cc2-b309-a85edf0f681b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño original:(280, 4096)\n",
            "Nuevo tamaño:(280, 4096)\n",
            "\n",
            "Tamaño original:(120, 4096)\n",
            "Nuevo tamaño:(120, 4096)\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train = preprocessing(X_train,y_train)\n",
        "print()\n",
        "X_test, y_test = preprocessing(X_test,y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRR9DAo1uy_Y"
      },
      "source": [
        "# Apartado b\n",
        "\n",
        "El perceptrón multicapa utiliza capas totalmente conectadas (densas) para realizar cálculos. El tamaño de un perceptrón para imágenes puede ser muy grande. Es por esto que utilizaremos PCA para reducir el número de dimensiones a algo más razonable. Generaremos dos conjuntos de datos utilizando las primeras 10 y 20 componentes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWwxTq3YZXgv"
      },
      "source": [
        "## PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYUogvUeu-KV"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca_10 = PCA(n_components=10)\n",
        "pca_20 = PCA(n_components=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8u9K0Sf8M8s"
      },
      "source": [
        "Vamos a generar un conjunto de datos con las primeras 10 componentes obtenidas con pca."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGjmvQeg7UEQ",
        "outputId": "5b864022-3aa8-4905-86e9-0a3ddcfb5b15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PCA(n_components=10)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pca_10.fit(X_train);\n",
        "\n",
        "X_train_10 = pca_10.transform(X_train);\n",
        "X_test_10 = pca_10.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98hCQ6ls8VSo"
      },
      "source": [
        "Vamos a generar otro conjunto de datos utilizando las primeras 20 componentes obtenidas con pca."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DdO6Yy92A9g"
      },
      "outputs": [],
      "source": [
        "pca_20.fit(X_train);\n",
        "\n",
        "X_train_20 = pca_20.transform(X_train);\n",
        "X_test_20 = pca_20.transform(X_test);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCIE_2TVbjFz",
        "outputId": "73ff8e08-744f-4d4c-ca6f-2863413d1dbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2800   1200\n",
            "5600   2400\n"
          ]
        }
      ],
      "source": [
        "print(X_train_10.size, ' ', X_test_10.size)\n",
        "print(X_train_20.size, ' ', X_test_20.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zlcViw28eqj"
      },
      "source": [
        "El siguiente código define un perceptrón con una capa oculta con función de activación ReLU. Fíjate en que la capa de salida es un softmax de tamaño 40 que permite predecir las probabilidades de cada clase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30-5BfCuZZPe"
      },
      "source": [
        "## Exploración parámetros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LECndZ0b5sK"
      },
      "source": [
        "Vamos a crear perceptrones con un valor de entrada igual a 10 o 20, en función de que conjunto de datos estemos utilizando. Como salida tendremos un softmax de tamaño 40, lo que nos permite predecir las probabilidades de cada una de las distintas clases. Para la capa oculta exploraremos el compoartamiento en cuanto a porcentaje de acierto en función del número de neuronas utilizadas (25, 50, 75 o 100).\n",
        "\n",
        "Una vez entrenado el modelo, predeciremos a que clase pertenece cada individuo del conjunto de test. Con el resultado, y a través de la función *np.argmax* veremos que ha predicho nuestro modelo. Comprobando esto con el conjunto *y_test* obtendremos el resultado de dicha red neuronal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtFsbx0qdgJq",
        "outputId": "ecdd9d6f-74a6-4c78-c0cf-9a9c44a13ace"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f30759454f0>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 8ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f30740ff1f0>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f30758c72b0>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 5ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f307574b8e0>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f30756ebc70>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3075639ee0>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3075605880>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 4ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f30754a9e50>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n"
          ]
        }
      ],
      "source": [
        "input_shape = [10,20]\n",
        "neurons = [25,50,75,100]\n",
        "predictions_results = []\n",
        "\n",
        "train = X_train_10\n",
        "test = X_test_10\n",
        "for x in input_shape:\n",
        "  if x == 20:\n",
        "    train = X_train_20\n",
        "    test = X_test_20\n",
        "  for y in neurons:\n",
        "    # create the model\n",
        "    model = keras.Sequential()\n",
        "    model.add(keras.Input(shape=(x)))\n",
        "    model.add(keras.layers.Dense(units=y, activation=\"relu\"))\n",
        "    model.add(keras.layers.Dense(40, activation=\"softmax\"))\n",
        "\n",
        "    # compile and fit the model\n",
        "    model.compile(optimizer=keras.optimizers.SGD(), loss=tf.keras.losses.CategoricalCrossentropy());\n",
        "    model.fit(train,y_train,batch_size=16, epochs=200,verbose=False);\n",
        "\n",
        "    # get the results for each unit neurons\n",
        "    pred = []\n",
        "    pred = model.predict(test);\n",
        "    predictions = pred.argmax(axis=1)\n",
        "\n",
        "    predictions_results.append(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMCD9YCCdCM_"
      },
      "source": [
        "Definimos una función auxiliar para calcular los aciertos de un conjunto de predicciones dadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By9kylgXRrgH"
      },
      "outputs": [],
      "source": [
        "def compute_accuracies(predictions):\n",
        "  accuracies = []\n",
        "  for i in range(0,len(predictions)):\n",
        "    correct_elem = 0\n",
        "    for j in range(0,predictions[i].size):\n",
        "\n",
        "      if y_test[j].argmax()+1 == predictions[i][j]:\n",
        "        correct_elem += 1\n",
        "      \n",
        "    accuracies.append((i,correct_elem/predictions[i].size))\n",
        "    \n",
        "  return accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQim-r36dKey"
      },
      "source": [
        "Vamos a ver los resultados de los distintos modelos entrenados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgxCyjliVp74",
        "outputId": "be1278c7-0a72-477f-d609-63cb9445a1f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "neurons 25-50-75-100 with 10 components:\n",
            "0.675\n",
            "0.6916666666666667\n",
            "0.6833333333333333\n",
            "0.7333333333333333\n",
            "\n",
            "neurons 25-50-75-100 with 20 components:\n",
            "0.7833333333333333\n",
            "0.7666666666666667\n",
            "0.825\n",
            "0.825\n"
          ]
        }
      ],
      "source": [
        "accuracies = compute_accuracies(predictions_results)\n",
        "\n",
        "print('neurons 25-50-75-100 with 10 components:')\n",
        "for x in accuracies:\n",
        "  if x[0] == 4:\n",
        "    print('\\nneurons 25-50-75-100 with 20 components:')\n",
        "  print(x[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWY1rLCvddfU"
      },
      "source": [
        "Podemos ver que parece haber un patrón tal que a mayor número de neuronas mejores resultados obtenemos en comparación a un menor número de estas. Como observación global, tanto con 10 como con 20 componentes, los mejores resultados se obtienen con 100 neuronas en la capa oculta.\n",
        "\n",
        "Destacar que el mejor resultado se da en el modelo entrenado con 20 componentes y donde el número de neuronas en la capa oculta es igual a 75 o 100 (0.825 de acierto).\n",
        "\n",
        "Un número bajo de neuronas puede derivas en *under-fitting* y, por el contrario, un gran número de neuronas puede conllevar un problema de *over-fitting*. Es por esto que, a pesar de que en nuestro caso ha sido así, no siempre un mayor número de neuronas corresponde a un mayor acierto en el conjunto testo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeKQOhp8u-w4"
      },
      "source": [
        "# Apartado c\n",
        "\n",
        "Una alternativa a los MLP son las capas convolucionales. Se trata de redes neuronales inspiradas en el funcionamiento de la corteza visual y especializadas en problemas de visión. Vamos a utilizar una capa convolucional para clasificar el conjunto de datos original. Como la entrada debe ser una matriz cuadrada, tendremos que transformar la forma de la matriz entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On4VRozggAw9"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reshape((280,64,64))\n",
        "X_test = X_test.reshape((120,64,64))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSNi5u8ogBFl"
      },
      "source": [
        "Las capas convolucionales se usan para procesar imágenes en color, por lo que asumen que cada imagen es una matriz 3D, la tercera dimensión es para los canales de color. En este caso, las imágenes son en escala de grises, por lo que tendremos que simular que tenemos una dimensión adicional. \n",
        "\n",
        "Vamos a utilizar la función reshape de numpy para transformar los datos de entrenamiento y test de tal manera que obtendremos como resultado una matriz de 4 dimensiones. La primera de estas son los ejemplos y las otras tres son la imagen de cada ejemplo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k0cEnWpgXAd",
        "outputId": "f24dce1d-c98b-4e7d-b3ab-d8ad4853d04e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(280, 64, 64, 1)\n"
          ]
        }
      ],
      "source": [
        "X_train = X_train.reshape(-1,64,64,1);\n",
        "X_test = X_test.reshape(-1,64,64,1);\n",
        "\n",
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG4re9AOgYW8"
      },
      "source": [
        "En este caso vamos a explorar el número de neuronas de la capa convolucional fijando el paso de las convoluciones (stride) a 1 y el tamaño del kernel de las convoluciones a 3. El número de neuronas que exploraremos será de 1 a 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cyEIYE-rUlu",
        "outputId": "963a9c18-1ba8-4195-de30-bd487845280b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3cd501fa60>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 18ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3cc9221c10>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 24ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3cc88b59d0>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 19ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3cc87e4550>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 18ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3cc872b550>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3cc86740a0>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 19ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3cc866d220>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 20ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3cc8779280>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 18ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3cc8436cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 18ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3cc835e2e0>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 18ms/step\n"
          ]
        }
      ],
      "source": [
        "neurons = [1,2,3,4,5,6,7,8,9,10]\n",
        "predictions_results = []\n",
        "\n",
        "for x in neurons:\n",
        "  # create the model\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.Input(shape=(64,64,1,)))\n",
        "  model.add(keras.layers.Conv2D(filters=x, kernel_size=3, strides=1, activation=\"relu\"))\n",
        "  model.add(keras.layers.Flatten())\n",
        "  model.add(keras.layers.Dense(40, activation=\"softmax\"))\n",
        "\n",
        "  # compile and fit the model\n",
        "  model.compile(optimizer=keras.optimizers.SGD(), loss=tf.keras.losses.CategoricalCrossentropy());\n",
        "  model.fit(X_train,y_train,batch_size=16, epochs=200,verbose=False);\n",
        "\n",
        "  # get the results for each unit neurons\n",
        "  pred = []\n",
        "  pred = model.predict(X_test);\n",
        "  predictions = pred.argmax(axis=1)\n",
        "\n",
        "  predictions_results.append(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsFsi3gpgcmM"
      },
      "source": [
        "Al igual que hemos hecho en el apartado anterior, y a través del mismo método, vamos a calcular el acierto para cada uno de los modelos entrenados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFFt53rtgdRF",
        "outputId": "321cfe78-da23-473c-a300-262de1de745e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neurons 1 : 0.875\n",
            "Neurons 2 : 0.8833333333333333\n",
            "Neurons 3 : 0.8833333333333333\n",
            "Neurons 4 : 0.8916666666666667\n",
            "Neurons 5 : 0.8833333333333333\n",
            "Neurons 6 : 0.8833333333333333\n",
            "Neurons 7 : 0.8916666666666667\n",
            "Neurons 8 : 0.8916666666666667\n",
            "Neurons 9 : 0.8916666666666667\n",
            "Neurons 10 : 0.8916666666666667\n"
          ]
        }
      ],
      "source": [
        "accuracies = compute_accuracies(predictions_results)\n",
        "\n",
        "for i in range(0,len(accuracies)):\n",
        "  print('Neurons', i+1, ':', accuracies[i][1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6RphjOfzF_U"
      },
      "source": [
        "Podemos ver como en los 10 modelos entrenados con capas convolucionales hemos obtenido mejores resultados que el mejor modelo del apartado anterior. Estos resultados no sorprenden ya que al fin y al cabo este tipo de modelos están especializados para trabajar la visión artificial. Son modelos pensados para trabajr con matrices de píxeles y que además permiten reduciar el coste computacional en gran medida.\n",
        "\n",
        "A partir de las 7 neuronas alcanzamos un acierto de casi el 0.892. Tal y como decíamos antes, si tuviéramos que escoger un modelo nos quedaríamos con uno de estos debido al mejor acierto que nos proporcionan, más de un 6% mejor. Cualquier modelo con 7, 8, 9 o 10 neuronas de los vistos en este apartado, sería la mejor elección.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEkMZmQlvvfR"
      },
      "source": [
        "# Apartado d\n",
        "\n",
        "El método summary() de la clase Model calcula cuántos parámetros tiene la red y también los parámetros por capa.\n",
        "\n",
        "La forma de calcular los tamaños y la cantidad de parámetros a calcular en una red depende del tipo de capas con el que estemos trabajando, en nuestro caso capas densas. El output de cada capa se define en la declaración de la red, por otro lado, el número de parámetros se calcula con la siguiente formula.\n",
        "$$\n",
        "num\\_param = num\\_output * (num\\_input + 1)\n",
        "$$\n",
        "\n",
        "El número de parámetros totales de una red, en nuestro caso, será la suma de los parámetros que hay en cada una de las redes."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a ver ahora el resumen de las capas que mejoes resultados nos han dado para cada uno de los tipos de redes con los que hemos trabajado. Un perceptrón multicapa y las capas convolucionales. El cálculo de los tamaños y los parámetros se obtienen internamente tal y como hemos explicado en el punto anterior."
      ],
      "metadata": {
        "id": "U0o2e7WUEutQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSB1zYXAv49J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6f9d4f9-5a63-4805-a519-2e2b1c9718a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 100)               2100      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 40)                4040      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,140\n",
            "Trainable params: 6,140\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = keras.Sequential(name='sequential')\n",
        "model.add(keras.Input(shape=(20)))\n",
        "model.add(keras.layers.Dense(units=100, activation=\"relu\", name='dense_1'))\n",
        "model.add(keras.layers.Dense(40, activation=\"softmax\", name='dense_2'))\n",
        "\n",
        "# summary also return the total number of parameters\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential(name='sequential')\n",
        "model.add(keras.Input(shape=(64,64,1,)))\n",
        "model.add(keras.layers.Conv2D(filters=10, kernel_size=3, strides=1, activation=\"relu\", name='conv2d_1'))\n",
        "model.add(keras.layers.Flatten(name='flatten_1'))\n",
        "model.add(keras.layers.Dense(40, activation=\"softmax\", name='dense_1'))\n",
        "\n",
        "# summary also return the total number of parameters\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbMfEu3AFKDA",
        "outputId": "721fef18-1d15-4201-cb6b-0f87795931d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_1 (Conv2D)           (None, 62, 62, 10)        100       \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 38440)             0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 40)                1537640   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,537,740\n",
            "Trainable params: 1,537,740\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential(name='sequential')\n",
        "model.add(keras.Input(shape=(64,64)))\n",
        "model.add(keras.layers.Dense(units=100, activation=\"relu\", name='dense_1'))\n",
        "model.add(keras.layers.Dense(40, activation=\"softmax\", name='dense_2'))\n",
        "\n",
        "# summary also return the total number of parameters\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiWr2xC4Bl61",
        "outputId": "d4c1d9ac-5335-40a1-a3cf-6d836273f018"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 64, 100)           6500      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64, 40)            4040      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,540\n",
            "Trainable params: 10,540\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero de todo recordar que con el primer modelo obtuvimos un acierto en el conjunto test de un 0.825 y con el segundo modelo de un 0.892.\n",
        "\n",
        "La principal, y clara, ventaja que tiene la segunda red respecto la primera es el porcentaje de acierto, al final, nuestro objetivo es conseguir crear un modelo capaz de predecir correctamente unas clases y por tanto, un mejor acierto siempre será mejor. La cuestión es, ¿és siempre rentable un mejor acierto? Podemos ver como el primero modelo tiene un número de parametros muchísimo más pequeño el segundo y esto acaba recayendo en que el la red convolucional, debido al alto número de parámetros, será mucho más costo que el primero, bastante más de hecho. En puntos como este, habría que debatir, en función del objetivo del problema, si perder algo de acierto es rentable en pos de un mejor rendimineto y menor costo computacional.\n",
        "\n",
        "Si nuestra entrada para la red MLP hubieran sido los datos originales (datos 64x64 en vez de utilizar componentes principales) el tamaño de la primera capa hubiera variado. La segunda capa sería igual ya que el tamaño de entrada seguiría siendo 100 y la salida 40. La primera capa en cambio tendría un tamaño de 6500 parámetros.\n",
        "$$\n",
        "num\\_param = num\\_output * (num\\_input + 1)\n",
        "$$\n",
        "$$\n",
        "6500 = 100*(64+1)\n",
        "$$\n",
        "\n",
        "Por tanto, el tamaño total de la red pasaría de 6140 a 10540, casi el doble. Por tanto, era importante este caso utilizar los componentes principales en vez de los datos en su forma original."
      ],
      "metadata": {
        "id": "8yh-T_r9FPod"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}